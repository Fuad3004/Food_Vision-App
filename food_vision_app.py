# -*- coding: utf-8 -*-
"""Food Vision App.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gEh3ptBGxzMZ0yRyPA0CrVatBf-1MW1N
"""

# Continue with regular imports
import matplotlib.pyplot as plt
import torch
import torchvision

from torch import nn
from torchvision import transforms

# Try to get torchinfo, install it if it doesn't work
try:
    from torchinfo import summary
except:
    print("[INFO] Couldn't find torchinfo... installing it.")
    !pip install -q torchinfo
    from torchinfo import summary

# Try to import the going_modular directory, download it from GitHub if it doesn't work
try:
    from python_file import data_setup, traintest
    from helper_functions import download_data, set_seeds, plot_loss_curves
except:
    # Get the going_modular scripts
    print("[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.")
    !git clone https://github.com/Fuad3004/ComupterVision-food101-Dataset.git
    !mv ComupterVision-food101-Dataset/python_file .

    !mv ComupterVision-food101-Dataset/helper_functions.py . # get the helper_functions.py script

    !rm -rf ComupterVision-food101-Dataset
    
    from python_file import data_setup, traintest
    from helper_functions import download_data, set_seeds, plot_loss_curves

device = "cuda" if torch.cuda.is_available() else "cpu"
device

import torchvision 
torchvision.__version__

"""#EffNetB2 Function Creating"""

def create_effnetb2_model(num_classes:int=5, 
                          seed:int=42):
    """Creates an EfficientNetB2 feature extractor model and transforms.

    Args:
        num_classes (int, optional): number of classes in the classifier head. 
            Defaults to 3.
        seed (int, optional): random seed value. Defaults to 42.

    Returns:
        model (torch.nn.Module): EffNetB2 feature extractor model. 
        transforms (torchvision.transforms): EffNetB2 image transforms.
    """
    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model
    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT
    transforms = weights.transforms()
    model = torchvision.models.efficientnet_b2(weights=weights)

    # 4. Freeze all layers in base model
    for param in model.parameters():
        param.requires_grad = False

    # 5. Change classifier head with random seed for reproducibility
    torch.manual_seed(seed)
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.3, inplace=True),
        nn.Linear(in_features=1408, out_features=num_classes), #I had to know about the value of in features! 
    )
    
    return model, transforms

# Create EffNetB2 model capable of fitting to 101 classes for Food101
effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)

from torchinfo import summary

# # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)
summary(effnetb2_food101, 
        input_size=(1, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])

"""#Getting Food Vision Data"""

# Create Food101 training data transforms (only perform data augmentation on the training images)
food101_train_transforms = torchvision.transforms.Compose([
    torchvision.transforms.TrivialAugmentWide(),
    effnetb2_transforms,
])

print(f"Training transforms:\n{food101_train_transforms}\n") 
print(f"Testing transforms:\n{effnetb2_transforms}")

from torchvision import datasets

# Setup data directory
from pathlib import Path
data_dir = Path("data")

# Get training data (~750 images x 101 food classes)
train_data_food101 = datasets.Food101(root=data_dir, # path to download data to
                              split="train", # dataset split to get
                              transform=food101_train_transforms, # perform data augmentation on training data
                              download=True) # want to download?

# Get testing data (~250 images x 101 food classes)
test_data_food101 = datasets.Food101(root=data_dir,
                             split="test",
                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data
                             download=True)

# Get Food101 class names
food101_class_names = train_data_food101.classes

# View the first 10
food101_class_names[:10]

"""#Turning our Food101 datasets into DataLoaders"""

import os
import torch

BATCH_SIZE = 32
NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs

# Create Food101 20 percent training DataLoader
train_dataloader_food101 = torch.utils.data.DataLoader(train_data_food101,
                                                                  batch_size=BATCH_SIZE,
                                                                  shuffle=True,
                                                                  num_workers=NUM_WORKERS)
# Create Food101 20 percent testing DataLoader
test_dataloader_food101 = torch.utils.data.DataLoader(test_data_food101,
                                                                 batch_size=BATCH_SIZE,
                                                                 shuffle=False,
                                                                 num_workers=NUM_WORKERS)

"""#Training the model using traintest file from"""

from python_file import traintest

# Setup optimizer
optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),
                             lr=1e-3)

# Setup loss function
loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes

# Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset
set_seeds()    
effnetb2_food101_results = traintest.train(model=effnetb2_food101,
                                        train_dataloader=train_dataloader_food101,
                                        test_dataloader=test_dataloader_food101,
                                        optimizer=optimizer,
                                        loss_fn=loss_fn,
                                        epochs=5,
                                        device=device)

"""#Saving the model"""

from python_file import saving

effnetb2_food101_model_path = "effnetb2_model_on_food_vision.pth"
# Save the model
saving.save_model(model=effnetb2_food101,
                 target_dir="models",
                 model_name=effnetb2_food101_model_path)

# Create Food101 compatible EffNetB2 instance
loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)

# Load the saved model's state_dict()
loaded_effnetb2_food101.load_state_dict(torch.load("models/effnetb2_model_on_food_vision.pth"))

from pathlib import Path

# Get the model size in bytes then convert to megabytes
pretrained_effnetb2_food101_model_size = Path("models", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) 
print(f"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB")



"""#Turning our FoodVision Big Model into A Deployable App"""

from pathlib import Path

# Create FoodVision Big demo path
foodvision_big_demo_path = Path("demos/foodvision/")

# Make FoodVision Big demo directory
foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)

# Make FoodVision Big demo examples directory
(foodvision_big_demo_path / "examples").mkdir(parents=True, exist_ok=True)

# Download and move an example image
!wget https://github.com/Fuad3004/Food_Vision-App/raw/main/Picture1.jpg
!mv Picture1.jpg demos/foodvision/examples/Picture1.jpg

# Move trained model to FoodVision Big demo folder (will error if model is already moved)
!mv models/effnetb2_model_on_food_vision.pth demos/foodvision

"""Saving Food101 class names to file"""

# Check out the first 10 Food101 class names
food101_class_names[:10]

# Create path to Food101 class names
foodvision_big_class_names_path = foodvision_big_demo_path / "class_names.txt"

# Write Food101 class names list to file
with open(foodvision_big_class_names_path, "w") as f:
    print(f"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}")
    f.write("\n".join(food101_class_names)) # leave a new line between each class

# Open Food101 class names file and read each line into a list
with open(foodvision_big_class_names_path, "r") as f:
    food101_class_names_loaded = [food.strip() for food in  f.readlines()]
    
# View the first 5 class names loaded back in
food101_class_names_loaded[:5]

"""#Turning our FoodVision Big model into a Python script (model.py)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile demos/foodvision/model.py
# import torch
# import torchvision
# 
# from torch import nn
# 
# 
# def create_effnetb2_model(num_classes:int=3, 
#                           seed:int=42):
#     """Creates an EfficientNetB2 feature extractor model and transforms.
# 
#     Args:
#         num_classes (int, optional): number of classes in the classifier head. 
#             Defaults to 3.
#         seed (int, optional): random seed value. Defaults to 42.
# 
#     Returns:
#         model (torch.nn.Module): EffNetB2 feature extractor model. 
#         transforms (torchvision.transforms): EffNetB2 image transforms.
#     """
#     # Create EffNetB2 pretrained weights, transforms and model
#     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT
#     transforms = weights.transforms()
#     model = torchvision.models.efficientnet_b2(weights=weights)
# 
#     # Freeze all layers in base model
#     for param in model.parameters():
#         param.requires_grad = False
# 
#     # Change classifier head with random seed for reproducibility
#     torch.manual_seed(seed)
#     model.classifier = nn.Sequential(
#         nn.Dropout(p=0.3, inplace=True),
#         nn.Linear(in_features=1408, out_features=num_classes),
#     )
#     
#     return model, transforms

pip install gradio

# Commented out IPython magic to ensure Python compatibility.
# %%writefile demos/foodvision/app.py
# ### 1. Imports and class names setup ### 
# import gradio as gr
# import os
# import torch
# 
# from model import create_effnetb2_model
# from timeit import default_timer as timer
# from typing import Tuple, Dict
# 
# # Setup class names
# with open("class_names.txt", "r") as f: # reading them in from class_names.txt
#     class_names = [food_name.strip() for food_name in  f.readlines()]
#     
# ### 2. Model and transforms preparation ###    
# 
# # Create model
# effnetb2, effnetb2_transforms = create_effnetb2_model(
#     num_classes=101, # could also use len(class_names)
# )
# 
# # Load saved weights
# effnetb2.load_state_dict(
#     torch.load(
#         f="effnetb2_model_on_food_vision.pth",
#         map_location=torch.device("cpu"),  # load to CPU
#     )
# )
# 
# ### 3. Predict function ###
# 
# # Create predict function
# def predict(img) -> Tuple[Dict, float]:
#     """Transforms and performs a prediction on img and returns prediction and time taken.
#     """
#     # Start the timer
#     start_time = timer()
#     
#     # Transform the target image and add a batch dimension
#     img = effnetb2_transforms(img).unsqueeze(0)
#     
#     # Put model into evaluation mode and turn on inference mode
#     effnetb2.eval()
#     with torch.inference_mode():
#         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities
#         pred_probs = torch.softmax(effnetb2(img), dim=1)
#     
#     # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)
#     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}
#     
#     # Calculate the prediction time
#     pred_time = round(timer() - start_time, 5)
#     
#     # Return the prediction dictionary and prediction time 
#     return pred_labels_and_probs, pred_time
# 
# ### 4. Gradio app ###
# 
# # Create title, description and article strings
# title = "FoodVision 🍰"
# description = "An EfficientNetB2 feature extractor computer vision model to classify images of food from 101 different classes {https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html}."
# article = "Please Visit this- https://github.com/Fuad3004/Food_Vision-App"
# 
# # Create examples list from "examples/" directory
# example_list = [["examples/" + example] for example in os.listdir("examples")]
# 
# # Create Gradio interface 
# demo = gr.Interface(
#     fn=predict,
#     inputs=gr.Image(type="pil"),
#     outputs=[
#         gr.Label(num_top_classes=5, label="Predictions"),
#         gr.Number(label="Prediction time (s)"),
#     ],
#     examples=example_list,
#     title=title,
#     description=description,
#     article=article,
# )
# 
# # Launch the app!
# demo.launch()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile demos/foodvision/requirements.txt
# torch==2.0.0
# torchvision==0.14.0
# gradio==3.30.0

torchvision.__version__

import gradio
gradio.__version__

"""#Zipping And Downloading"""

# Zip foodvision_big folder but exclude certain files
!cd demos/foodvision && zip -r ../foodvision.zip * -x "*.pyc" "*.ipynb" "*__pycache__*" "*ipynb_checkpoints*"

# Download the zipped FoodVision Big app (if running in Google Colab)
try:
    from google.colab import files
    files.download("demos/foodvision.zip")
except:
    print("Not running in Google Colab, can't use google.colab.files.download()")

